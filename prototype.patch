diff --git a/pkg/virt-launcher/virtwrap/manager.go b/pkg/virt-launcher/virtwrap/manager.go
index 63d8d43d2..a7dbb2296 100644
--- a/pkg/virt-launcher/virtwrap/manager.go
+++ b/pkg/virt-launcher/virtwrap/manager.go
@@ -97,6 +97,9 @@ const (
 const maxConcurrentHotplugHostDevices = 1
 const maxConcurrentMemoryDumps = 1
 
+const hotplugDetachmentGracePeriod = 5 * time.Second
+const hotplugDetachmentVerbosity = 1
+
 type contextStore struct {
 	ctx    context.Context
 	cancel context.CancelFunc
@@ -142,6 +145,7 @@ type LibvirtDomainManager struct {
 
 	hotplugHostDevicesInProgress chan struct{}
 	memoryDumpInProgress         chan struct{}
+	attachDiskChannels           map[string]chan struct{}
 
 	virtShareDir             string
 	ephemeralDiskDir         string
@@ -204,6 +208,7 @@ func newLibvirtDomainManager(connection cli.Connection, virtShareDir, ephemeralD
 		cancelSafetyUnfreezeChan: make(chan struct{}),
 		migrateInfoStats:         &stats.DomainJobInfo{},
 		metadataCache:            metadataCache,
+		attachDiskChannels:       make(map[string]chan struct{}),
 	}
 
 	manager.hotplugHostDevicesInProgress = make(chan struct{}, maxConcurrentHotplugHostDevices)
@@ -922,19 +927,68 @@ func (l *LibvirtDomainManager) SyncVMI(vmi *v1.VirtualMachineInstance, allowEmul
 		return nil, err
 	}
 
+	detachedDisks := getDetachedDisks(vmi.Name, vmi.Namespace, oldSpec.Devices.Disks, domain.Spec.Devices.Disks)
+	for attachDiskChannelName, attachDiskChannel := range l.attachDiskChannels {
+		if _, ok := detachedDisks[attachDiskChannelName]; !ok {
+			logger.V(hotplugDetachmentVerbosity).Infof("Hotplug detach stil requested, continue grace period = [%+v]", attachDiskChannelName)
+		} else {
+			// If disk is no longer in detachedDisk list, it no longer needs to be detached.  Signal
+			// detach cancellation and remove from detachedDisks list.
+			logger.V(hotplugDetachmentVerbosity).Infof("Hotplug detach not longer needed, cancel it [%+v]", attachDiskChannelName)
+			// Signal detach goutine to cancel
+			attachDiskChannel <- struct{}{}
+			// Remove from detachedDisks list so detach goutine is not recreated
+			delete(detachedDisks, attachDiskChannelName)
+		}
+	}
 	// Look up all the disks to detach
-	for _, detachDisk := range getDetachedDisks(oldSpec.Devices.Disks, domain.Spec.Devices.Disks) {
-		logger.V(1).Infof("Detaching disk %s, target %s", detachDisk.Alias.GetName(), detachDisk.Target.Device)
+	for namespacedDiskName, detachDisk := range detachedDisks {
+		detachDiskName := detachDisk.Alias.GetName()
+		detachDiskTargetDevice := detachDisk.Target.Device
+		domainName := domain.Spec.Name
+
 		detachBytes, err := xml.Marshal(detachDisk)
 		if err != nil {
 			logger.Reason(err).Error("marshalling detached disk failed")
 			return nil, err
 		}
-		err = dom.DetachDeviceFlags(strings.ToLower(string(detachBytes)), affectLiveAndConfigLibvirtFlags)
-		if err != nil {
-			logger.Reason(err).Error("detaching device")
-			return nil, err
-		}
+
+		// Create goroutine to do detachment so we can wait to see if the disk is reattached.
+		go func() {
+			logger.V(hotplugDetachmentVerbosity).Infof("Potentially detaching hotplug disk %s", namespacedDiskName)
+
+			// If channel has already been created, a goroutine is already waiting for the disk to be detached.  Exit
+			// so there aren't multiple goroutines waiting to detach the same disk.
+			if _, ok := l.attachDiskChannels[namespacedDiskName]; ok {
+				logger.V(hotplugDetachmentVerbosity).Infof("Potential hotplug disk detaching already handled: %s", namespacedDiskName)
+				return
+			}
+			// There are no other goroutines waiting to detach this disk, create a channel enable subsequent calls to
+			// SyncVMI to cancel this detach (if attach is requested within the grace period: 5 seconds)
+			l.attachDiskChannels[namespacedDiskName] = make(chan struct{})
+			// Wait for either the grace period to expire (in which case: proceed to detach) or for the attach channel
+			// to be triggered (in which case: cancel the detach)
+			select {
+			case <-l.attachDiskChannels[namespacedDiskName]:
+				// Disk attach has been requested within the grace period, cancel the detach
+				logger.V(hotplugDetachmentVerbosity).Infof("Potential hotplug disk detaching was cancelled: %s", namespacedDiskName)
+			case <-time.After(5 * time.Second):
+				// Grace period has expired without an attache request, proceed to detach
+				logger.V(1).Infof("Detaching disk %s, target %s", detachDiskName, detachDiskTargetDevice)
+				dom, err := l.virConn.LookupDomainByName(domainName)
+				if err != nil {
+					logger.Reason(err).Error("getting domain for device detach")
+					break
+				}
+
+				err = dom.DetachDeviceFlags(strings.ToLower(string(detachBytes)), affectLiveAndConfigLibvirtFlags)
+				if err != nil {
+					logger.Reason(err).Error("detaching device")
+				}
+			}
+			// Detach request has been handled, remove channel from map
+			delete(l.attachDiskChannels, namespacedDiskName)
+		}()
 	}
 	// Look up all the disks to attach
 	for _, attachDisk := range getAttachedDisks(oldSpec.Devices.Disks, domain.Spec.Devices.Disks) {
@@ -945,7 +999,18 @@ func (l *LibvirtDomainManager) SyncVMI(vmi *v1.VirtualMachineInstance, allowEmul
 		if !allowAttach {
 			continue
 		}
-		logger.V(1).Infof("Attaching disk %s, target %s", attachDisk.Alias.GetName(), attachDisk.Target.Device)
+		attachDiskName := attachDisk.Alias.GetName()
+		namespacedDiskName := fmt.Sprintf("%s_%s_%s", vmi.Namespace, vmi.Name, attachDiskName)
+		if _, ok := l.attachDiskChannels[namespacedDiskName]; ok {
+			logger.V(hotplugDetachmentVerbosity).Infof("Hotplug attach during detach grace period, signal detach goroutine: %s", namespacedDiskName)
+			// a goroutine is waiting to detach this disk, signal it to stop
+			l.attachDiskChannels[namespacedDiskName] <- struct{}{}
+			// no need to reattach if the disk is already attached
+			logger.V(hotplugDetachmentVerbosity).Infof("Skip hotplug detach/reattach disk: %s", namespacedDiskName)
+			continue
+		}
+
+		logger.V(1).Infof("Attaching disk %s, target %s", attachDiskName, attachDisk.Target.Device)
 		// set drivers cache mode
 		err = converter.SetDriverCacheMode(&attachDisk, l.directIOChecker)
 		if err != nil {
@@ -1032,7 +1097,7 @@ func isHotplugDisk(disk api.Disk) bool {
 	return strings.HasPrefix(getSourceFile(disk), v1.HotplugDiskDir)
 }
 
-func getDetachedDisks(oldDisks, newDisks []api.Disk) []api.Disk {
+func getDetachedDisks(vmiName, vmiNamespace string, oldDisks, newDisks []api.Disk) map[string]api.Disk {
 	newDiskMap := make(map[string]api.Disk)
 	for _, disk := range newDisks {
 		file := getSourceFile(disk)
@@ -1040,14 +1105,15 @@ func getDetachedDisks(oldDisks, newDisks []api.Disk) []api.Disk {
 			newDiskMap[file] = disk
 		}
 	}
-	res := make([]api.Disk, 0)
+	res := map[string]api.Disk{}
 	for _, oldDisk := range oldDisks {
 		if !isHotplugDisk(oldDisk) {
 			continue
 		}
 		if _, ok := newDiskMap[getSourceFile(oldDisk)]; !ok {
 			// This disk got detached, add it to the list
-			res = append(res, oldDisk)
+			namespacedDiskName := fmt.Sprintf("%s_%s_%s", vmiNamespace, vmiName, oldDisk.Alias.GetName())
+			res[namespacedDiskName] = oldDisk
 		}
 	}
 	return res
